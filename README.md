# ğŸ›¡ï¸ ToxiGuard AI

> A smart, privacy-respecting browser extension that flags, explains, and optionally censors toxic language in real-time.

ToxiGuard AI uses TensorFlow.js and the `toxicity` NLP model to detect harmful or offensive content while you're typing on any website.

---

## ğŸš€ Features

- ğŸ” **Real-time toxicity detection** in text inputs, textareas, and contenteditable elements
- ğŸ¯ **Fine-grain category controls** - Enable/disable specific toxicity types
- ğŸ§  **Smart categorization** - Highlights specific toxicity categories (insult, threat, obscene, etc.)
- ğŸ˜¶ **Visual feedback system** - Emoji indicators and border highlighting
- âœ… **Auto-censor functionality** - Optionally replaces toxic content with asterisks
- âš™ï¸ **Adjustable sensitivity** - Customizable detection threshold (0.1 - 1.0)
- ğŸ§° **Advanced popup controls** - Toggle detection and configure all settings
- âŒ¨ï¸ **Keyboard shortcuts** - Alt+T to quickly toggle detection
- ğŸ’¾ **Persistent settings** - Your preferences are saved automatically
- ğŸ›¡ï¸ **Cross-browser compatible** - Works on Chrome, Edge, and Firefox (Manifest V3)
- ğŸ¨ **Modern UI** - Beautiful, intuitive interface with smooth animations

---

## ğŸ› ï¸ Installation

1. Clone this repository or [Download ZIP](https://github.com/VKrishna04/ToxicGuard_AI/archive/main.zip)
   ```powershell
   git clone https://github.com/VKrishna04/ToxicGuard_AI.git
   cd ToxicGuard_AI
   ```

2. Go to your browser's extensions page:
   - **Chrome**: `chrome://extensions`
   - **Edge**: `edge://extensions`
   - **Firefox**: `about:debugging` â†’ **This Firefox**

3. Enable **Developer Mode** (top-right toggle)

4. Click **Load unpacked** and select the `ToxicGuard_AI/` folder

5. The extension icon should appear in your toolbar

---

## ğŸ’¡ How It Works

```mermaid
flowchart TD
    A[User types into a text field] --> B[Content script detects input]
    B --> C{Detection enabled?}
    C -->|No| D[Skip processing]
    C -->|Yes| E[Load TensorFlow.js model]
    E --> F[Classify text with enabled categories]
    F --> G{Toxic content detected?}
    G -->|Yes| H[Show red border + tooltip]
    G -->|No| I[Show clean state]
    H --> J{Auto-censor enabled?}
    J -->|Yes| K[Replace toxic words with ***]
    J -->|No| L[Keep original text]
    H --> M[Update emoji indicator to ğŸ˜ ]
    I --> N[Update emoji indicator to ğŸ˜Š]
```

---

## ğŸ§ª Testing the Extension

1. **Open the test page**: Load `test.html` in your browser
2. **Try these examples**:

   **Safe content:**
   - `Hello, how are you doing today?`
   - `This is a great project, keep up the good work!`
   - `I love learning new things about technology`

   **Toxic content:**
   - `You're an idiot`
   - `This is shit`
   - `I hate you so much`

3. **Watch for feedback**:
   - ğŸ”´ Red borders around toxic inputs
   - ğŸ’¬ Tooltips showing detected categories
   - ğŸ˜Š/ğŸ˜  Emoji indicator (bottom-right)
   - â­ Auto-censoring (if enabled)

---

## âš™ï¸ Settings & Controls

### Main Controls
- **ğŸ”˜ Enable Detection** â€“ Master on/off switch
- **ğŸ”˜ Auto Censor** â€“ Replace toxic words with asterisks

### Sensitivity Settings
- **ğŸšï¸ Detection Threshold** â€“ Adjustable from 0.1 (very sensitive) to 1.0 (less sensitive)
- **Default**: 0.9 (recommended for balanced detection)

### Toxicity Categories
Fine-tune which types of content to detect:

| Category              | Icon  | Description                                 |
| --------------------- | ----- | ------------------------------------------- |
| **Insults**           | ğŸ¤¬    | Personal attacks and offensive name-calling |
| **Obscene**           | ğŸ”    | Profanity and vulgar language               |
| **Threats**           | âš”ï¸    | Threatening language and intimidation       |
| **Identity Attacks**  | ğŸ¯    | Attacks based on identity/group membership  |
| **Sexually Explicit** | ğŸ”    | Sexual content and explicit material        |
| **Severe Toxicity**   | ğŸ’€    | Extremely harmful and toxic content         |

### Keyboard Shortcuts
- **Alt + T** â€“ Quick toggle detection on/off

---

## ğŸ§  Technical Details

### Model Information
- **Framework**: [TensorFlow.js](https://js.tensorflow.org/)
- **Model**: [`@tensorflow-models/toxicity`](https://github.com/tensorflow/tfjs-models/tree/master/toxicity)
- **Architecture**: Pre-trained transformer model
- **Categories**: 6 distinct toxicity types
- **Performance**: ~500ms initial load, <100ms per classification

### Architecture
```
ğŸ“ ToxicGuard_AI/
â”œâ”€â”€ ğŸ“„ manifest.json      # Extension configuration
â”œâ”€â”€ ğŸ“„ background.js      # Service worker (settings, shortcuts)
â”œâ”€â”€ ğŸ“„ content.js         # Main detection logic
â”œâ”€â”€ ğŸ“„ popup.html         # Settings interface
â”œâ”€â”€ ğŸ“„ popup.js           # Popup functionality
â”œâ”€â”€ ğŸ“„ test.html          # Testing page
â”œâ”€â”€ ğŸ“ icons/             # Extension icons
â””â”€â”€ ğŸ“„ README.md          # This file
```

### Browser Compatibility
| Browser | Version | Status                         |
| ------- | ------- | ------------------------------ |
| Chrome  | 88+     | âœ… Full support                |
| Edge    | 88+     | âœ… Full support                |
| Firefox | 109+    | âœ… Full support                |
| Safari  | -       | âŒ Not supported (Manifest V3) |

---

## ğŸ”§ Development

### Setup for Development
```powershell
# Clone the repository
git clone https://github.com/VKrishna04/ToxicGuard_AI.git
cd ToxicGuard_AI

# Install development dependencies (if any)
# Currently uses CDN imports, no build process needed

# Load in browser for testing
# Follow installation steps above
```

### Testing
1. **Load extension** in developer mode
2. **Open test.html** for comprehensive testing
3. **Check console** for debug information
4. **Test on real websites** like Reddit, Twitter, etc.

### Contributing
1. Fork the repository
2. Create a feature branch: `git checkout -b feature-name`
3. Make your changes
4. Test thoroughly
5. Submit a pull request

---

## ğŸš§ Future Enhancements

- ğŸŒ **Multilingual support** - Detect toxicity in multiple languages
- ğŸ§‘â€ğŸ“ **Educational tooltips** - Explain why content is considered toxic
- ğŸ“Š **Statistics dashboard** - Track toxicity patterns over time
- ğŸ”„ **AI-powered suggestions** - Suggest alternative, positive phrasing
- ğŸŒ **Website-specific settings** - Custom rules for different sites
- ğŸ“± **Mobile browser support** - Extend to mobile browsers
- ğŸ¤– **Custom model training** - Fine-tune for specific use cases
- ğŸ”’ **Privacy enhancements** - Local-only processing options

---

## ğŸ“Š Performance

- **Model size**: ~25MB (loaded once per session)
- **Memory usage**: ~50MB RAM
- **Processing time**: 50-200ms per text classification
- **Debounce delay**: 500ms (configurable)
- **Battery impact**: Minimal (only processes on user input)

---

## ğŸ›¡ï¸ Privacy & Security

- âœ… **No data collection** - Everything runs locally
- âœ… **No external API calls** - Uses browser-native TensorFlow.js
- âœ… **No user tracking** - No analytics or telemetry
- âœ… **Open source** - Code is fully auditable
- âœ… **Minimal permissions** - Only storage and notifications

---

## ğŸ–¼ï¸ Screenshots

### Extension Popup
![Popup Interface](icons/hero.png)

### Detection in Action
*Red borders appear around toxic content with category tooltips*

### Settings Panel
*Fine-grain controls for each toxicity category*

---

## âš–ï¸ License

This project is licensed under the [Apache 2.0 License](https://opensource.org/licenses/Apache-2.0).

```
Copyright 2025 &copy; VKrishna04

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
```

---

## ğŸ‘¨â€ğŸ’» Author

**VKrishna04**
- ğŸ™ GitHub: [@VKrishna04](https://github.com/VKrishna04)
- ğŸ¢ Organization: [Life-Experimentalists](https://github.com/orgs/Life-Experimentalists/repositories)
- ğŸŒ Website: [VKrishna04.github.io](https://vkrishna04.github.io)
- ğŸ”— Portfolio: [VKrishna04.me](https://vkrishna04.me)

---

## ğŸ™ Acknowledgments

- [TensorFlow.js Team](https://www.tensorflow.org/js) for the amazing ML framework
- [Perspective API](https://perspectiveapi.com/) for inspiration
- [Jigsaw](https://jigsaw.google.com/) for toxicity research
- The open-source community for feedback and contributions

---

<div align="center">

**Made with â¤ï¸ for a better internet**

[â­ Star this project](https://github.com/VKrishna04/ToxicGuard_AI) | [ğŸ› Report Issues](https://github.com/VKrishna04/ToxicGuard_AI/issues) | [ğŸ’¡ Request Features](https://github.com/VKrishna04/ToxicGuard_AI/issues/new)

</div>
